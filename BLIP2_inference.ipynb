{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import glob\n",
    "import torch\n",
    "import wandb\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "\n",
    "# some helper constants and functions\n",
    "MODEL_ID = \"Salesforce/blip2-opt-2.7b\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def load_image(path: str):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID, torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_captioning(image: Image.Image):\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[\n",
    "        0\n",
    "    ].strip()\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompted_image_captioning(image: Image.Image, prompt: str):\n",
    "    inputs = processor(image, text=prompt, return_tensors=\"pt\").to(\n",
    "        device, torch.float16\n",
    "    )\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[\n",
    "        0\n",
    "    ].strip()\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqa(image: Image.Image, question: str):\n",
    "    inputs = processor(image, text=question, return_tensors=\"pt\").to(\n",
    "        device, torch.float16\n",
    "    )\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=10)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[\n",
    "        0\n",
    "    ].strip()\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_based_prompting(\n",
    "    image: Image.Image, context: List[Tuple[str, str]], question: str\n",
    "):\n",
    "    template = \"Question: {} Answer: {}.\"\n",
    "    prompt = (\n",
    "        \" \".join(\n",
    "            [template.format(context[i][0], context[i][1]) for i in range(len(context))]\n",
    "        )\n",
    "        + \" Question: \"\n",
    "        + question\n",
    "        + \" Answer:\"\n",
    "    )\n",
    "    inputs = processor(image, text=prompt, return_tensors=\"pt\").to(\n",
    "        device, torch.float16\n",
    "    )\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=10)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[\n",
    "        0\n",
    "    ].strip()\n",
    "    return generated_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image captioning table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all images in the images folder\n",
    "image_paths = glob.glob(\"images/*.png\")\n",
    "wandb.init(project=\"BLIP-2\", name=\"image_captioning\")\n",
    "table = wandb.Table(columns=[\"Image\",\"Generated caption\"])\n",
    "\n",
    "for img in image_paths:\n",
    "    image = load_image(img)\n",
    "    caption = image_captioning(image)\n",
    "    table.add_data(wandb.Image(image), caption)\n",
    "\n",
    "wandb.log({\"img_captioning\": table})\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompted Image captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all images in the images folder\n",
    "image_paths = glob.glob(\"images/*.png\")\n",
    "wandb.init(project=\"BLIP-2\", name=\"image_captioning\")\n",
    "table = wandb.Table(columns=[\"Prompt\", \"Image\",\"Generated caption\"])\n",
    "\n",
    "for img in image_paths:\n",
    "    image = load_image(img)\n",
    "    caption = image_captioning(image)\n",
    "    table.add_data(wandb.Image(image), caption)\n",
    "\n",
    "wandb.log({\"img_captioning\": table})\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "c0a39285c03d41d3f5ae5bf2c9c125287338cbad4cc5fed1b7e72698fe592576"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
